"""Command-line interface for plant disease detection."""

import json
import logging
import os
import sys
from pathlib import Path

import click
import torch

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.utils.io import (
    setup_logging,
    set_seed,
    load_config,
    save_config,
    save_json,
    ensure_dir,
    get_device,
    validate_image_file,
)
from src.data.dataset_utils import (
    load_dataset_from_folders,
    load_dataset_from_csv,
    split_dataset,
    PlantDiseaseDataset,
    create_dataloaders,
    compute_class_weights,
)
from src.data.augmentations import get_train_transforms, get_val_transforms
from src.models.model import create_model, get_supported_backbones
from src.models.train import (
    Trainer,
    EarlyStopping,
    create_optimizer,
    create_scheduler,
    create_criterion,
)
from src.models.inference import PlantDiseasePredictor, save_predictions
from src.eval.metrics import evaluate_model, compute_metrics, save_evaluation_report
from src.explainability.gradcam import GradCAM

logger = logging.getLogger("plant_disease_detector")


@click.group()
def main():
    """Plant Disease Detector CLI."""
    pass


@main.command()
@click.option('--data', required=True, help='Path to dataset directory or CSV file')
@click.option('--output', required=True, help='Output directory for model and logs')
@click.option('--model', default='resnet50', help='Model backbone (default: resnet50)')
@click.option('--epochs', default=30, help='Number of training epochs')
@click.option('--batch', default=32, help='Batch size')
@click.option('--lr', default=0.0001, type=float, help='Learning rate')
@click.option('--freeze-epochs', default=0, help='Epochs to freeze backbone')
@click.option('--early-stopping', default=5, help='Early stopping patience')
@click.option('--config', default=None, help='Path to config YAML file')
@click.option('--gpu/--no-gpu', default=True, help='Use GPU if available')
@click.option('--seed', default=42, help='Random seed')
def train(data, output, model, epochs, batch, lr, freeze_epochs, early_stopping, config, gpu, seed):
    """Train a plant disease detection model."""
    
    # Setup
    ensure_dir(output)
    setup_logging(log_level='INFO', log_file=os.path.join(output, 'train.log'))
    set_seed(seed)
    
    logger.info("=" * 80)
    logger.info("PLANT DISEASE DETECTOR - TRAINING")
    logger.info("=" * 80)
    
    # Load config
    if config:
        cfg = load_config(config)
    else:
        # Default config
        config_path = Path(__file__).parent.parent / 'config.yaml'
        if config_path.exists():
            cfg = load_config(str(config_path))
        else:
            cfg = {}
    
    # Override config with CLI arguments
    cfg['seed'] = seed
    cfg['batch_size'] = batch
    cfg['epochs'] = epochs
    cfg['backbone'] = model
    cfg['freeze_backbone_epochs'] = freeze_epochs
    cfg.setdefault('optimizer', {})['lr'] = lr
    
    # Save config
    save_config(cfg, os.path.join(output, 'config.yaml'))
    
    # Device
    device = get_device(gpu)
    
    # Load dataset
    logger.info(f"Loading dataset from {data}")
    
    if data.endswith('.csv'):
        image_paths, labels, class_names = load_dataset_from_csv(data)
        
        # Split dataset
        train_paths, train_labels, val_paths, val_labels, test_paths, test_labels = split_dataset(
            image_paths, labels,
            train_ratio=cfg.get('data_split', {}).get('train', 0.8),
            val_ratio=cfg.get('data_split', {}).get('val', 0.1),
            test_ratio=cfg.get('data_split', {}).get('test', 0.1),
            seed=seed
        )
    else:
        # Check if pre-split
        train_dir = os.path.join(data, 'train')
        val_dir = os.path.join(data, 'val')
        
        if os.path.exists(train_dir) and os.path.exists(val_dir):
            # Pre-split dataset
            logger.info("Detected pre-split dataset")
            train_paths, train_labels, class_names = load_dataset_from_folders(train_dir)
            val_paths, val_labels, _ = load_dataset_from_folders(val_dir)
            test_paths, test_labels = [], []
            
            test_dir = os.path.join(data, 'test')
            if os.path.exists(test_dir):
                test_paths, test_labels, _ = load_dataset_from_folders(test_dir)
        else:
            # Single folder - need to split
            image_paths, labels, class_names = load_dataset_from_folders(data)
            train_paths, train_labels, val_paths, val_labels, test_paths, test_labels = split_dataset(
                image_paths, labels, seed=seed
            )
    
    num_classes = len(class_names)
    logger.info(f"Classes: {class_names}")
    logger.info(f"Number of classes: {num_classes}")
    
    # Save class names
    class_names_path = os.path.join(output, 'class_names.txt')
    with open(class_names_path, 'w') as f:
        for name in class_names:
            f.write(f"{name}\n")
    
    # Create datasets
    image_size = tuple(cfg.get('image_size', [224, 224]))
    
    train_transform = get_train_transforms(
        image_size=image_size,
        **cfg.get('augmentation', {})
    )
    val_transform = get_val_transforms(image_size=image_size)
    
    train_dataset = PlantDiseaseDataset(train_paths, train_labels, class_names, train_transform)
    val_dataset = PlantDiseaseDataset(val_paths, val_labels, class_names, val_transform)
    
    # Create data loaders
    train_loader, val_loader, _ = create_dataloaders(
        train_dataset, val_dataset, None,
        batch_size=batch,
        num_workers=cfg.get('num_workers', 4)
    )
    
    # Create model
    logger.info(f"Creating model: {model}")
    net = create_model(
        num_classes=num_classes,
        backbone=model,
        pretrained=cfg.get('pretrained', True),
        dropout=cfg.get('dropout', 0.3),
        freeze_backbone=(freeze_epochs > 0),
        device=device
    )
    
    # Create optimizer
    optimizer = create_optimizer(
        net,
        optimizer_name=cfg.get('optimizer', {}).get('name', 'adam'),
        lr=lr,
        weight_decay=cfg.get('optimizer', {}).get('weight_decay', 0.0001)
    )
    
    # Create scheduler
    scheduler_cfg = cfg.get('scheduler', {})
    scheduler = create_scheduler(optimizer, **scheduler_cfg)
    
    # Create criterion
    class_weights = None
    if cfg.get('class_weights') == 'auto':
        class_weights = compute_class_weights(train_labels, num_classes)
    
    criterion = create_criterion(
        loss_name=cfg.get('loss', 'cross_entropy'),
        class_weights=class_weights,
        label_smoothing=cfg.get('label_smoothing', 0.0),
